[
  {
    "objectID": "vis03_colors.html",
    "href": "vis03_colors.html",
    "title": "Colors",
    "section": "",
    "text": "Color is one of the most important thing to consider when working with plots.\nDealing with colors in R can be a bit tricky. There are numerous packages that attempt to make it easier, however, each one works a bit different. At the core, colors can be either defined by their name or by their RGB “value”:\nYou can get the name of the 657 predefined colors by calling the colors() function.\n\ncolors()\n\nHere is a little function that shows a random color.\n\ncolorsample = function(){\n    random_color = sample(colors(), 1)\n\nggplot()+\n    labs(title = random_color)+\n    theme(panel.background = element_rect(fill = random_color))\n}\n\ncolorsample()\n\nThe second option for colors is using the additive color model. Here we define the amount of red, green and blue (and optionally alpha i.e. transparency) each with two digit hexadecimal numbers.\n\n\n\n\n\n\nHexadecimal numbers\n\n\n\nare base 16 numbers. We have 16 options for each digit instead of the usual 10. The symbols we use for these 16 options are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F. So FF is the highest possible two digit hexadecimal number corresponding to 255 in base 10.\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\ndf = read.csv(\"data/crop_species.csv\")\n\nggplot(df, aes(x = CaraInd, y = CaraSpec))+\n    geom_point(color = \"firebrick\")+\n    theme(panel.background = element_rect(fill = \"#50FFFF50\"))\n\n\n\n\n\n\n\n\n\n\nColor scales are usually just defined as a series of colors.\n\nlibrary(viridis)\nviridis(5)\n\n[1] \"#440154FF\" \"#3B528BFF\" \"#21908CFF\" \"#5DC863FF\" \"#FDE725FF\"\n\n\n\n\nCode\nggplot(data.frame(x = seq(5), y = 1), aes(x, y, fill = x))+\n    geom_raster()+\n    scale_fill_gradientn(colors = viridis(5), guide = \"none\")+\n    scale_x_continuous(labels = viridis(5), breaks = seq(5), name = NULL, expand = c(0,0))+\n    scale_y_continuous(name = NULL, breaks = NULL, expand = c(0,0))+\n    theme(axis.text.x = element_text(size = 12, color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\nDepending on the data you want to visualize, different colorscale types are needed for adequate plots. Here, I go over some examples of commonly used colorscales and options:\n\n\n\n… are for continuous data, i.e. numeric values like the amount of Species. Here I use scale_color_viridis_c(), the c stands for continuous.\n\nggplot(df, aes(x = CaraInd, y = CaraSpec, color = CaraSpec))+\n    geom_point(size = 3)+\n    scale_color_viridis_c()\n\n\n\n\n\n\n\n\n\n\n\n… groups numeric values together into bins instead of following a gradient:\n\nggplot(df, aes(x = CaraInd, y = CaraSpec, color = CaraSpec))+\n    geom_point(size = 3)+\n    scale_color_viridis_b()\n\n\n\n\n\n\n\n\n\n\n\n… do not work with continuous, i.e. numeric data, but need discrete values like the croptype:\n\nggplot(df, aes(x = CaraInd, y = CaraSpec, color = Croptype))+\n    geom_point(size = 3)+\n    scale_color_viridis_d()\n\n\n\n\n\n\n\n\n\n\n\n… are similar to continuous scales, but have a neutral value from which colors diverge. A common example is a anomaly or error that can go in either a positive or negative direction. In order to get easy access to diverging scales, I use the colorspace package here.\n\nlibrary(colorspace)\n\ndf$AraAnomanly = df$AraInd - mean(df$AraInd)\n\nggplot(df, aes(x = plotcode, y = AraAnomanly, color = AraAnomanly))+\n    geom_point(size = 3)+\n    colorspace::scale_color_continuous_diverging()+\n    theme_light()",
    "crumbs": [
      "Graphics",
      "Colors"
    ]
  },
  {
    "objectID": "vis03_colors.html#color-scales",
    "href": "vis03_colors.html#color-scales",
    "title": "Colors",
    "section": "",
    "text": "Color scales are usually just defined as a series of colors.\n\nlibrary(viridis)\nviridis(5)\n\n[1] \"#440154FF\" \"#3B528BFF\" \"#21908CFF\" \"#5DC863FF\" \"#FDE725FF\"\n\n\n\n\nCode\nggplot(data.frame(x = seq(5), y = 1), aes(x, y, fill = x))+\n    geom_raster()+\n    scale_fill_gradientn(colors = viridis(5), guide = \"none\")+\n    scale_x_continuous(labels = viridis(5), breaks = seq(5), name = NULL, expand = c(0,0))+\n    scale_y_continuous(name = NULL, breaks = NULL, expand = c(0,0))+\n    theme(axis.text.x = element_text(size = 12, color = \"black\"))\n\n\n\n\n\n\n\n\n\n\n\nDepending on the data you want to visualize, different colorscale types are needed for adequate plots. Here, I go over some examples of commonly used colorscales and options:\n\n\n\n… are for continuous data, i.e. numeric values like the amount of Species. Here I use scale_color_viridis_c(), the c stands for continuous.\n\nggplot(df, aes(x = CaraInd, y = CaraSpec, color = CaraSpec))+\n    geom_point(size = 3)+\n    scale_color_viridis_c()\n\n\n\n\n\n\n\n\n\n\n\n… groups numeric values together into bins instead of following a gradient:\n\nggplot(df, aes(x = CaraInd, y = CaraSpec, color = CaraSpec))+\n    geom_point(size = 3)+\n    scale_color_viridis_b()\n\n\n\n\n\n\n\n\n\n\n\n… do not work with continuous, i.e. numeric data, but need discrete values like the croptype:\n\nggplot(df, aes(x = CaraInd, y = CaraSpec, color = Croptype))+\n    geom_point(size = 3)+\n    scale_color_viridis_d()\n\n\n\n\n\n\n\n\n\n\n\n… are similar to continuous scales, but have a neutral value from which colors diverge. A common example is a anomaly or error that can go in either a positive or negative direction. In order to get easy access to diverging scales, I use the colorspace package here.\n\nlibrary(colorspace)\n\ndf$AraAnomanly = df$AraInd - mean(df$AraInd)\n\nggplot(df, aes(x = plotcode, y = AraAnomanly, color = AraAnomanly))+\n    geom_point(size = 3)+\n    colorspace::scale_color_continuous_diverging()+\n    theme_light()",
    "crumbs": [
      "Graphics",
      "Colors"
    ]
  },
  {
    "objectID": "R09_linearmodels.html",
    "href": "R09_linearmodels.html",
    "title": "Linear Models",
    "section": "",
    "text": "Linear models describe the relation between two variables. In a non-technical description we can think about linear model like:\n\nFind a line that is as close as possible to all the points of a scatterplot.\n\n\n\n\n\n\n\n\n\n\nFor example, we can use a linear model to depict the relation between the amount of samples animals and the number of different species found in a given area. To create a linear model, we need two vectors (here as two columns of a data.frame) with the matching data pairs (Abundance and Species Number). The lm function then takes a formula with the vector/column names separated by ~. You can interpret the ~ in this case as a function of. So lm(Species ~ Abundance) means: Give me the number of species as a function of species abundance.\n\nlibrary(dplyr)\nbd = read.csv(\"data/crop_species.csv\")\n\nbd = bd %&gt;% mutate(Abundance = AraInd + CaraInd + IsoMyrInd,\n              Species = AraSpec, CaraSpec, IsoMyrSpec)\n\nlmod = lm(Species ~ Abundance, bd)\nlmod\n\n\nCall:\nlm(formula = Species ~ Abundance, data = bd)\n\nCoefficients:\n(Intercept)    Abundance  \n   10.71651      0.01749  \n\n\n\n\nCalling the model returns the intercept and slope. To get a more useful output use the summary function. In the following we will go over the output of the summary function on linear models. The explanations are mostly taken from this blog entry: https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R\n\nsummary(lmod)\n\n\nCall:\nlm(formula = Species ~ Abundance, data = bd)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3136 -1.2481 -0.2235  1.1245  5.3046 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.716507   0.991039  10.813 1.58e-15 ***\nAbundance    0.017491   0.003656   4.784 1.22e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.416 on 58 degrees of freedom\nMultiple R-squared:  0.2829,    Adjusted R-squared:  0.2706 \nF-statistic: 22.89 on 1 and 58 DF,  p-value: 1.221e-05\n\n\n\n\n\nthe function that was used to create the model\n\n\n\n\n\nsummary statistic of the model error\n“How far away are the real values from the model?”\nshould follow a normal distribution (e.g. by plotting a histogram)\n\n\n\n\n\n\n\n\n\n\n\nhist(lmod$residuals)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nagain the slope and intercept\nwe can expect to observed at least 10.7 different species on a plot (if we collect 103 animals)\nfor each additional animal observed, the species count increases by 0.017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe expected error of the coefficients\nfor each additional animal observed, the species number increases by 0.017 +- 0.0037\n\n\n\n\n\nhow many standard deviation is the std. error away from zero?\nfar away from zero is better. Reduces the probability that the result is due to chance\nusually not interpreted\n\n\n\n\n\nprobability that the observed value is larger than t\nsmall value means that there is a high chance that the relation is not based on chance\np values &lt; than 0.05 is the convention that we can neglect the H0 hypothesis (the model actually depicts a relation)\n\n\n\n\n\na legend for the significance level of the p-value\n\n\n\n\n\nthe average deviation of the real values from the model value\ndegree of freedom: on how many observations is this value based on?\n\n\nactual_values = bd$Species\npredicted_values = predict(lmod, data.frame(Abundance = bd$Abundance))\n\n\n\n\n\n\n\n\n\n\nFor the specific Abundance values we have in our observed data, we can calculate the error i.e. the deviation of the predicted values from the actual values:\n\n# deviation between actual values and predicted values (\"error\")\nresiduals = actual_values - predicted_values\n\n# squared residuals: no negative values, large error values are more severe\nsquared_error = (actual_values - predicted_values)**2\n\nHowever, we also want to give estimates of the models error for abundance values that we have not observed.\n\n# number of observations\nn = length(actual_values)\n\n\n# mean squared error\nmean(squared_error)\n\n[1] 5.643017\n\n# mean squared error with degrees of freedom\nsum(squared_error)/(n-1)\n\n[1] 5.738662\n\n# root mean squared error\nsqrt(mean(squared_error))\n\n[1] 2.375504\n\n# root mean squared error with degrees of freedom == Residuals standard error\nsqrt(sum(squared_error)/(n-2))\n\n[1] 2.416113\n\n\n\n\n\n\nHow much of the variance is explained by the model?\nValues between 0 and 1\n0 means no explanation, 1 means perfect fit\n\n\n# how much variation is there in the observed values\ntotal_sum_of_squares = sum(((actual_values) - mean(actual_values))**2)\n\n\n# how much variation is there in the residuals\nresidual_sum_of_squares = sum((actual_values - predicted_values)**2)\n\n\n# divide one by the other:\n# if both values are similar, this approaches 1\n# if residual sum of squares is very low (i.e. the model is good) this value gets smaller\nresidual_sum_of_squares / total_sum_of_squares\n\n[1] 0.7170542\n\n# Rsq: values close to 1 mean, the residual sum of squares error is low compared to the observed variation\n1 - residual_sum_of_squares / total_sum_of_squares\n\n[1] 0.2829458\n\n\n\n\n\n\nIs there a relation between the predictor and the response?\nIs my model better than if all the coefficients are zero?\nAway from 1 is good!\nDepend on number of data points.\n\n\n\n\n\n\nPlotting the linear model gives you 4 different figures that indicate if the model is statistically valid in the first place, i.e. if the residuals follow a normal distribution and if they are homoscedatic.\n\n\n\nIs it actually a linear relation?\nPoints randomly scattered around a horizontal line indicates this\n\n\n\n\n\nAre the residuals normally distributed?\nPoints on the 1-1 line indicates this\nDepicted are the actual residuals vs. a theoretical normal distribution\n\n\n\n\n\nAre the residuals homoscedatic?\nThis means that the range of the residuals are more or less equal for the whole data range\nEqually spread points around the line indicates this\n\n\n\n\n\nAre there outliers in the data that have influence on the model?\nIn this example, if we would leave out point #12, the model outcome would change.\n\n\nplot(lmod)",
    "crumbs": [
      "Data analysis",
      "Linear Models"
    ]
  },
  {
    "objectID": "R09_linearmodels.html#linear-model-summary",
    "href": "R09_linearmodels.html#linear-model-summary",
    "title": "Linear Models",
    "section": "",
    "text": "Calling the model returns the intercept and slope. To get a more useful output use the summary function. In the following we will go over the output of the summary function on linear models. The explanations are mostly taken from this blog entry: https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R\n\nsummary(lmod)\n\n\nCall:\nlm(formula = Species ~ Abundance, data = bd)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3136 -1.2481 -0.2235  1.1245  5.3046 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.716507   0.991039  10.813 1.58e-15 ***\nAbundance    0.017491   0.003656   4.784 1.22e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.416 on 58 degrees of freedom\nMultiple R-squared:  0.2829,    Adjusted R-squared:  0.2706 \nF-statistic: 22.89 on 1 and 58 DF,  p-value: 1.221e-05\n\n\n\n\n\nthe function that was used to create the model\n\n\n\n\n\nsummary statistic of the model error\n“How far away are the real values from the model?”\nshould follow a normal distribution (e.g. by plotting a histogram)\n\n\n\n\n\n\n\n\n\n\n\nhist(lmod$residuals)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nagain the slope and intercept\nwe can expect to observed at least 10.7 different species on a plot (if we collect 103 animals)\nfor each additional animal observed, the species count increases by 0.017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe expected error of the coefficients\nfor each additional animal observed, the species number increases by 0.017 +- 0.0037\n\n\n\n\n\nhow many standard deviation is the std. error away from zero?\nfar away from zero is better. Reduces the probability that the result is due to chance\nusually not interpreted\n\n\n\n\n\nprobability that the observed value is larger than t\nsmall value means that there is a high chance that the relation is not based on chance\np values &lt; than 0.05 is the convention that we can neglect the H0 hypothesis (the model actually depicts a relation)\n\n\n\n\n\na legend for the significance level of the p-value\n\n\n\n\n\nthe average deviation of the real values from the model value\ndegree of freedom: on how many observations is this value based on?\n\n\nactual_values = bd$Species\npredicted_values = predict(lmod, data.frame(Abundance = bd$Abundance))\n\n\n\n\n\n\n\n\n\n\nFor the specific Abundance values we have in our observed data, we can calculate the error i.e. the deviation of the predicted values from the actual values:\n\n# deviation between actual values and predicted values (\"error\")\nresiduals = actual_values - predicted_values\n\n# squared residuals: no negative values, large error values are more severe\nsquared_error = (actual_values - predicted_values)**2\n\nHowever, we also want to give estimates of the models error for abundance values that we have not observed.\n\n# number of observations\nn = length(actual_values)\n\n\n# mean squared error\nmean(squared_error)\n\n[1] 5.643017\n\n# mean squared error with degrees of freedom\nsum(squared_error)/(n-1)\n\n[1] 5.738662\n\n# root mean squared error\nsqrt(mean(squared_error))\n\n[1] 2.375504\n\n# root mean squared error with degrees of freedom == Residuals standard error\nsqrt(sum(squared_error)/(n-2))\n\n[1] 2.416113\n\n\n\n\n\n\nHow much of the variance is explained by the model?\nValues between 0 and 1\n0 means no explanation, 1 means perfect fit\n\n\n# how much variation is there in the observed values\ntotal_sum_of_squares = sum(((actual_values) - mean(actual_values))**2)\n\n\n# how much variation is there in the residuals\nresidual_sum_of_squares = sum((actual_values - predicted_values)**2)\n\n\n# divide one by the other:\n# if both values are similar, this approaches 1\n# if residual sum of squares is very low (i.e. the model is good) this value gets smaller\nresidual_sum_of_squares / total_sum_of_squares\n\n[1] 0.7170542\n\n# Rsq: values close to 1 mean, the residual sum of squares error is low compared to the observed variation\n1 - residual_sum_of_squares / total_sum_of_squares\n\n[1] 0.2829458\n\n\n\n\n\n\nIs there a relation between the predictor and the response?\nIs my model better than if all the coefficients are zero?\nAway from 1 is good!\nDepend on number of data points.",
    "crumbs": [
      "Data analysis",
      "Linear Models"
    ]
  },
  {
    "objectID": "R09_linearmodels.html#linear-model-plot",
    "href": "R09_linearmodels.html#linear-model-plot",
    "title": "Linear Models",
    "section": "",
    "text": "Plotting the linear model gives you 4 different figures that indicate if the model is statistically valid in the first place, i.e. if the residuals follow a normal distribution and if they are homoscedatic.\n\n\n\nIs it actually a linear relation?\nPoints randomly scattered around a horizontal line indicates this\n\n\n\n\n\nAre the residuals normally distributed?\nPoints on the 1-1 line indicates this\nDepicted are the actual residuals vs. a theoretical normal distribution\n\n\n\n\n\nAre the residuals homoscedatic?\nThis means that the range of the residuals are more or less equal for the whole data range\nEqually spread points around the line indicates this\n\n\n\n\n\nAre there outliers in the data that have influence on the model?\nIn this example, if we would leave out point #12, the model outcome would change.\n\n\nplot(lmod)",
    "crumbs": [
      "Data analysis",
      "Linear Models"
    ]
  },
  {
    "objectID": "df04_wide_long.html",
    "href": "df04_wide_long.html",
    "title": "Wide vs. Long data.frames",
    "section": "",
    "text": "Most data you collect yourself or can download from data repositories will come in some variation or form of a “Excel” style spreadsheet. What I mean by that is, that a row in the table contains different features of the same observation. Each column on the other hand contains the same feature of many different observations. Of course, what information we store with feature and observation heavily depends on the content and purpose of the data.\nTake a look at the following data from the German weather service DWD about sunshine duration across German states. You can get this data here. I will only use a subset of the data:\n\nlibrary(dplyr)\n\nsunshine = sunshine |&gt;\n    filter(Jahr &gt; 2015 & Jahr &lt;= 2020) |&gt;\n    select(\"Jahr\", \"Sachsen\", \"Hessen\", \"Bayern\")\n\n\n\n\n\n\n\n\n\n\n\nJahr\nSachsen\nHessen\nBayern\n\n\n\n\n2016\n649.2\n579.4\n638.5\n\n\n2017\n682.9\n608.2\n720.3\n\n\n2018\n772.1\n774.0\n779.6\n\n\n2019\n791.2\n769.3\n787.0\n\n\n2020\n653.0\n640.2\n695.6\n\n\n\n\n\n\n\nTake a conscious look on this data.frame for a moment.\n\nWhat information does one row contain?\nWhat is stored in one column?\nWhat is the meaning of the numbers in the data.frame? (And maybe more importantly: How do you know?)\n\n\nThis typical format is called wide as we have information (i.e. the sunshine duration) stored in different columns for different states per observation year (the row). While this is a perfectly fine form of data storage (e.g. you can get a good overview of the data), there is a second organisation scheme that can also be very useful, especially in the context of data analysis and visualization with R: the long format.\n\n\n\nThe principle of the long format is, that we have one column where we store the actual measurement values (sunshine duration in this example). Along that, we have multiple columns with metadata that describe and “encode” each value. Take a look at the example data from above in the long format:\n\n\n\n\n\n\n\n\nJahr\nBundesland\nSonnenscheindauer\n\n\n\n\n2016\nSachsen\n649.2\n\n\n2016\nHessen\n579.4\n\n\n2016\nBayern\n638.5\n\n\n2017\nSachsen\n682.9\n\n\n2017\nHessen\n608.2\n\n\n2017\nBayern\n720.3\n\n\n2018\nSachsen\n772.1\n\n\n2018\nHessen\n774.0\n\n\n2018\nBayern\n779.6\n\n\n2019\nSachsen\n791.2\n\n\n2019\nHessen\n769.3\n\n\n2019\nBayern\n787.0\n\n\n2020\nSachsen\n653.0\n\n\n2020\nHessen\n640.2\n\n\n2020\nBayern\n695.6\n\n\n\n\n\n\n\nAll the measurement of sunshine duration from the different states are now in one column Sonnenscheindauer. And the two columns Jahr and Bundesland describe each value. In a way, the long format is better for the overall understanding than the wide format as it is way clearer what the content of each column actually means.\n\n\n\n\nlibrary(tidyr)\n\nsunshine_long = pivot_longer(sunshine,\n                             cols = c(\"Sachsen\", \"Hessen\", \"Bayern\"),\n                             names_to = \"Bundesland\",\n                             values_to = \"Sonnenscheindauer\")\n\n\n\n\n\n\n\n\n\nJahr\nBundesland\nSonnenscheindauer\n\n\n\n\n2016\nSachsen\n649.2\n\n\n2016\nHessen\n579.4\n\n\n2016\nBayern\n638.5\n\n\n2017\nSachsen\n682.9\n\n\n2017\nHessen\n608.2\n\n\n2017\nBayern\n720.3\n\n\n2018\nSachsen\n772.1\n\n\n2018\nHessen\n774.0\n\n\n2018\nBayern\n779.6\n\n\n2019\nSachsen\n791.2\n\n\n2019\nHessen\n769.3\n\n\n2019\nBayern\n787.0\n\n\n2020\nSachsen\n653.0\n\n\n2020\nHessen\n640.2\n\n\n2020\nBayern\n695.6\n\n\n\n\n\n\n\n\nsunshine_wide = pivot_wider(sunshine_long,\n                            names_from = \"Bundesland\",\n                            values_from = \"Sonnenscheindauer\")\n\n\n\n\n\n\n\n\n\nJahr\nSachsen\nHessen\nBayern\n\n\n\n\n2016\n649.2\n579.4\n638.5\n\n\n2017\n682.9\n608.2\n720.3\n\n\n2018\n772.1\n774.0\n779.6\n\n\n2019\n791.2\n769.3\n787.0\n\n\n2020\n653.0\n640.2\n695.6\n\n\n\n\n\n\n\n\nsunshine_year = pivot_wider(sunshine_long,\n                            names_from = \"Jahr\",\n                            values_from = \"Sonnenscheindauer\")\n\n\n\n\n\n\n\n\n\nBundesland\n2016\n2017\n2018\n2019\n2020\n\n\n\n\nSachsen\n649.2\n682.9\n772.1\n791.2\n653.0\n\n\nHessen\n579.4\n608.2\n774.0\n769.3\n640.2\n\n\nBayern\n638.5\n720.3\n779.6\n787.0\n695.6\n\n\n\n\n\n\n\nIf you want to go into more details for pivot_wider and pivot_longer check out this tidyr reference article.",
    "crumbs": [
      "Data handling",
      "Wide vs. Long data.frames"
    ]
  },
  {
    "objectID": "df04_wide_long.html#wide-format",
    "href": "df04_wide_long.html#wide-format",
    "title": "Wide vs. Long data.frames",
    "section": "",
    "text": "Jahr\nSachsen\nHessen\nBayern\n\n\n\n\n2016\n649.2\n579.4\n638.5\n\n\n2017\n682.9\n608.2\n720.3\n\n\n2018\n772.1\n774.0\n779.6\n\n\n2019\n791.2\n769.3\n787.0\n\n\n2020\n653.0\n640.2\n695.6\n\n\n\n\n\n\n\nTake a conscious look on this data.frame for a moment.\n\nWhat information does one row contain?\nWhat is stored in one column?\nWhat is the meaning of the numbers in the data.frame? (And maybe more importantly: How do you know?)\n\n\nThis typical format is called wide as we have information (i.e. the sunshine duration) stored in different columns for different states per observation year (the row). While this is a perfectly fine form of data storage (e.g. you can get a good overview of the data), there is a second organisation scheme that can also be very useful, especially in the context of data analysis and visualization with R: the long format.",
    "crumbs": [
      "Data handling",
      "Wide vs. Long data.frames"
    ]
  },
  {
    "objectID": "df04_wide_long.html#long-format",
    "href": "df04_wide_long.html#long-format",
    "title": "Wide vs. Long data.frames",
    "section": "",
    "text": "The principle of the long format is, that we have one column where we store the actual measurement values (sunshine duration in this example). Along that, we have multiple columns with metadata that describe and “encode” each value. Take a look at the example data from above in the long format:\n\n\n\n\n\n\n\n\nJahr\nBundesland\nSonnenscheindauer\n\n\n\n\n2016\nSachsen\n649.2\n\n\n2016\nHessen\n579.4\n\n\n2016\nBayern\n638.5\n\n\n2017\nSachsen\n682.9\n\n\n2017\nHessen\n608.2\n\n\n2017\nBayern\n720.3\n\n\n2018\nSachsen\n772.1\n\n\n2018\nHessen\n774.0\n\n\n2018\nBayern\n779.6\n\n\n2019\nSachsen\n791.2\n\n\n2019\nHessen\n769.3\n\n\n2019\nBayern\n787.0\n\n\n2020\nSachsen\n653.0\n\n\n2020\nHessen\n640.2\n\n\n2020\nBayern\n695.6\n\n\n\n\n\n\n\nAll the measurement of sunshine duration from the different states are now in one column Sonnenscheindauer. And the two columns Jahr and Bundesland describe each value. In a way, the long format is better for the overall understanding than the wide format as it is way clearer what the content of each column actually means.",
    "crumbs": [
      "Data handling",
      "Wide vs. Long data.frames"
    ]
  },
  {
    "objectID": "df04_wide_long.html#conversion-between-long-and-wide-with-tidyr",
    "href": "df04_wide_long.html#conversion-between-long-and-wide-with-tidyr",
    "title": "Wide vs. Long data.frames",
    "section": "",
    "text": "library(tidyr)\n\nsunshine_long = pivot_longer(sunshine,\n                             cols = c(\"Sachsen\", \"Hessen\", \"Bayern\"),\n                             names_to = \"Bundesland\",\n                             values_to = \"Sonnenscheindauer\")\n\n\n\n\n\n\n\n\n\nJahr\nBundesland\nSonnenscheindauer\n\n\n\n\n2016\nSachsen\n649.2\n\n\n2016\nHessen\n579.4\n\n\n2016\nBayern\n638.5\n\n\n2017\nSachsen\n682.9\n\n\n2017\nHessen\n608.2\n\n\n2017\nBayern\n720.3\n\n\n2018\nSachsen\n772.1\n\n\n2018\nHessen\n774.0\n\n\n2018\nBayern\n779.6\n\n\n2019\nSachsen\n791.2\n\n\n2019\nHessen\n769.3\n\n\n2019\nBayern\n787.0\n\n\n2020\nSachsen\n653.0\n\n\n2020\nHessen\n640.2\n\n\n2020\nBayern\n695.6\n\n\n\n\n\n\n\n\nsunshine_wide = pivot_wider(sunshine_long,\n                            names_from = \"Bundesland\",\n                            values_from = \"Sonnenscheindauer\")\n\n\n\n\n\n\n\n\n\nJahr\nSachsen\nHessen\nBayern\n\n\n\n\n2016\n649.2\n579.4\n638.5\n\n\n2017\n682.9\n608.2\n720.3\n\n\n2018\n772.1\n774.0\n779.6\n\n\n2019\n791.2\n769.3\n787.0\n\n\n2020\n653.0\n640.2\n695.6\n\n\n\n\n\n\n\n\nsunshine_year = pivot_wider(sunshine_long,\n                            names_from = \"Jahr\",\n                            values_from = \"Sonnenscheindauer\")\n\n\n\n\n\n\n\n\n\nBundesland\n2016\n2017\n2018\n2019\n2020\n\n\n\n\nSachsen\n649.2\n682.9\n772.1\n791.2\n653.0\n\n\nHessen\n579.4\n608.2\n774.0\n769.3\n640.2\n\n\nBayern\n638.5\n720.3\n779.6\n787.0\n695.6\n\n\n\n\n\n\n\nIf you want to go into more details for pivot_wider and pivot_longer check out this tidyr reference article.",
    "crumbs": [
      "Data handling",
      "Wide vs. Long data.frames"
    ]
  }
]